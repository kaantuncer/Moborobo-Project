# ROS Camera LIDAR Calibration Package

## Setup

Install dependencies.

```
sudo apt install ros-DISTRO-camera-calibration
```

Run the following to clone the `lidar_camera_calibration` package in `ros_workspace/src` directory.

```
cd ~/ros_workspace/src
git clone https://github.com/heethesh/lidar_camera_calibration

cd ~/ros_workspace/
catkin_make
source devel/setup.bash
```

Make sure you have the ROS bag file in `lidar_camera_calibration/bagfiles` folder. Then you can use the following launch files. This package assumes that the bag file has atleast the following topic names and message types by default, these can be modified in the launch scripts.

```
/sensors/velodyne_points    (sensor_msgs/PointCloud2)
/sensors/camera/image_color (sensor_msgs/Image)
/sensors/camera/camera_info (sensor_msgs/CameraInfo) (optionally generated by camera_calibration, see below)
```

## Play ROS Bag File

This launch file will only play the rosbag record file.

```
roslaunch lidar_camera_calibration play_rosbag.launch bagfile:=/path/to/file.bag
```

## Update the ROS Bag File

This script will update the camera matrices and the distortion coefficients in the `/sensors/camera/camera_info` topic and creates a new bag file in the same location. Note, if you did not have any camera calibration information before, ROS would automatically pick the camera info from `~/.ros/camera_info` when playing the bag file and you can skip this step after verification (`rostopic echo /<CAMERA_NAME>/camera_info`).

```
rosrun lidar_camera_calibration update_camera_info.py <original_file.bag> <calibration_file.yaml>
```

## Calibrate Camera-LiDAR Point Correspondences

This script will perform calibration using the matplotlib GUI to pick correspondences in the camera and the LiDAR frames. You first need to play the rosbag record in another terminal.

```
roslaunch lidar_camera_calibration play_rosbag.launch bagfile:=/path/to/file.bag
rosrun lidar_camera_calibration calibrate_camera_lidar.py --calibrate
```
```
Attention:
  -Topic names must be checked.
  -Sensor output shape must be checked. Output shape must be in 2D.
  -NaN values should be avoided.
  -While calculating error in calibrate function, shape of error variable must be checked. It must be in 2D.
  -The same number of points must be selected from both windows. The same rotation should be used when making the selection.

```

Press [ENTER] to launch the GUIs and pick the corresponding points by selecting the four corner points of the checkerboard in both the camera and the LiDAR frames.

OpenCV's PnP RANSAC + refinement using LM is used to find the rotation and translation transforms between the camera and the LiDAR. Since OpenCV's function rectifies the images internally, the 2D points are picked from the unrectified image. Additional, the `rectify` flag can be set to `True` while creating the GUI process to pick points from a rectified image.


**NOTE: The point files are appended and the extrinsics estimates are calculated and refined continuously using a RANSAC approach.**

**NOTE: To use `solvePnPRefineLM`, you need OpenCV >= 4.1.1, otherwise the LM pose refinement step will be skipped.**

The point correspondences are saved as following:
- Image Points: `lidar_camera_calibration/calibration_data/lidar_camera_calibration/img_corners.npy`
- LiDAR Points: `lidar_camera_calibration/calibration_data/lidar_camera_calibration/pcl_corners.npy`

The calibrated extrinsics are saved as following:
- `lidar_camera_calibration/calibration_data/lidar_camera_calibration/extrinsics.npz`
    - 'euler' : Euler Angles (RPY rad)
    - 'R'     : Rotation Matrix
    - 'T'     : Translation Offsets (XYZ m)

The following calibrated extrinsics were obtained:

#### Rotation Matrix
```
[[-0.01898458 -0.99916866 -0.03607741]
 [-0.07639216  0.03742802 -0.99637512]
 [ 0.9968971  -0.01615973 -0.07703921]]

```

#### Euler Angles (RPY rad)

```
(-2.9348305394545777, -1.4919990130059257, -1.8143765800459426)

```

#### Translation Offsets (XYZ m)

```
[[ 0.12115746  0.037129   -0.19124162]]

```
